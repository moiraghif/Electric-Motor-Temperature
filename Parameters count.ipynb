{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# First Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(mod):\n",
    "    return np.sum(list(np.prod(l.cpu().detach().numpy().shape) for l in mod.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def print_params(files):\n",
    "    for name, file in files.items():\n",
    "        params = np.sum([count_parameters(torch.load(\"./models/best_model_\" + f))\n",
    "                       for f in file])\n",
    "        print(name, params, sep=\": \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class Custom_RNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, recur,\n",
    "                 regressors,\n",
    "                 p_dropout=0.3):\n",
    "        self.recur_input, self.recur_output = recur\n",
    "        # recur_input in [0, len(hiddens))\n",
    "        # recur_output in [-1, len(hiddens)]\n",
    "        assert 0 <= self.recur_input <= len(hiddens)\n",
    "        assert -1 <= self.recur_output < len(hiddens)\n",
    "        assert len(regressors) == 2\n",
    "\n",
    "        super(Custom_RNN, self).__init__()\n",
    "        # input_shape = len(FEATURES)  # new task\n",
    "        input_shape = len(FEATURES) + len(TARGET)  # old task\n",
    "        self.previous_size = hiddens[self.recur_output] \\\n",
    "            if self.recur_output >= 0 else len(TARGET)\n",
    "        \n",
    "        hiddens_in = [input_shape] + hiddens\n",
    "\n",
    "        hiddens_in[self.recur_input] += self.previous_size\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=p_dropout)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_shape, out_shape) \\\n",
    "             for in_shape, out_shape in zip(hiddens_in, hiddens)])\n",
    "        self.layers_fn = [torch.nn.functional.relu \\\n",
    "                          for _ in self.layers]\n",
    "\n",
    "        regressor_list = []\n",
    "        for features, regressor in zip([1, 3], regressors):\n",
    "            if type(regressor) != list:\n",
    "                regressor = [regressor]\n",
    "            input_shape = [hiddens[-1]] + regressor\n",
    "            y_net = [torch.nn.ReLU() for _ in input_shape + regressor]\n",
    "            y_net[::2] = [torch.nn.Linear(in_shape, out_shape)\n",
    "                          for in_shape, out_shape\n",
    "                          in zip(input_shape, regressor + [features])]\n",
    "            regressor_list.append(torch.nn.Sequential(*y_net))\n",
    "        self.regressors = torch.nn.ModuleList(regressor_list)\n",
    "\n",
    "    def initialize_prev(self, x):\n",
    "        return torch.zeros(len(x), self.previous_size).to(DEVICE)\n",
    "\n",
    "    def forward(self, data, lag=None):\n",
    "        _, l, _ = data.shape\n",
    "        x = data[:, 0, :]  # .reshape(-1, len(FEATURES))\n",
    "        if lag is None:  # first recursion\n",
    "            lag = self.initialize_prev(x)\n",
    "        for i, (layer, fn) in enumerate(zip(self.layers, self.layers_fn)):\n",
    "            if i == self.recur_input:\n",
    "                x = torch.cat([x, lag], axis=1)\n",
    "            x = fn(layer(x))\n",
    "            x = self.dropout(x)\n",
    "            if i == self.recur_output and l > 1:\n",
    "                return self.forward(data[:, 1:, :], x)\n",
    "        x = torch.cat([regressor(x) for regressor in self.regressors],\n",
    "                      axis=1)\n",
    "        if self.recur_output == -1 and l > 1:\n",
    "            return self.forward(data[:, 1:, :], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_Net_2(nn.Module):  # AML_CNN, cella 11\n",
    "    def __init__(self, batch, in_c, out, \n",
    "                 filtro1, filtro2,\n",
    "                 num_ser, \n",
    "                 neuroni1, neuroni2,\n",
    "                 kernel1, kernel2, kernel3, kernel4, \n",
    "                 #padding1, padding2, padding3, padding4, \n",
    "                 stride1, stride2, stride3, stride4):\n",
    "        super(CNN_Net_2, self).__init__()\n",
    "        self.batch_size = 1\n",
    "        self.in_c = in_c\n",
    "        l0 = num_ser\n",
    "        l1 = outputSize(l0,kernel1,stride1,0)\n",
    "        l2 = outputSize(l1,kernel2,stride2,0)\n",
    "        l3 = outputSize(l2,kernel3,stride3,0)\n",
    "        l4 = outputSize(l3,kernel4,stride4,0)\n",
    "        #print(l4)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels= in_c, \n",
    "                               out_channels= filtro1, \n",
    "                               kernel_size= kernel1,\n",
    "                               stride= stride1,\n",
    "                               padding=0)    \n",
    "        self.pool1 = nn.MaxPool1d(kernel2,stride=stride2,padding=0) \n",
    "        self.conv2 = nn.Conv1d(filtro1,filtro2,kernel3,stride=stride3,padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel4,stride=stride4,padding=0)\n",
    "        self.fc1 = nn.Linear(filtro2*l4, neuroni1)\n",
    "        self.fc2 = nn.Linear(neuroni1, neuroni2)\n",
    "        self.fc3 = nn.Linear(neuroni2, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _ = x.shape\n",
    "        x =  self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(batch_size, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return  self.fc3(x)\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:       # Get the products\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class GRU_Net(nn.Module):  # AML_GRU, cella 7\n",
    "    def __init__(self, features, hidden, h1, h2, out):\n",
    "        super(GRU_Net, self).__init__()\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "\n",
    "        self.hidden_size = hidden\n",
    "        self.features = features\n",
    "\n",
    "        self.gru = nn.GRU(self.features, self.hidden_size, 1, #feature_size, hidden_size, num_layer\n",
    "                            batch_first = True) \n",
    "        self.fc1 = nn.Linear(self.hidden_size, \n",
    "                                h1)\n",
    "        self.out = nn.Linear(h1, out)\n",
    "\n",
    "        if self.h2 != 0:\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.out = nn.Linear(h2, out)\n",
    "        \n",
    "        #we do it stateless so there is no need for the hidden_state\n",
    "        #self.hidden = None #torch.randn(1, ??, self.hidden) #num_layer, batch, hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _ = x.shape \n",
    "        \n",
    "        x, _ =  self.gru(x)\n",
    "        x = F.tanh(x[:,-1].view(batch_size, -1))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.h2 != 0:\n",
    "            x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_Net(nn.Module):  # AML_LSTM, cella 9\n",
    "    def __init__(self, features, hidden, h1, h2, out):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "\n",
    "        self.hidden_size = hidden\n",
    "        self.features = features\n",
    "\n",
    "        self.lstm = nn.LSTM(self.features, self.hidden_size, 1, #feature_size, hidden_size, num_layer\n",
    "                            batch_first = True) \n",
    "        self.fc1 = nn.Linear(self.hidden_size, \n",
    "                                h1)\n",
    "        self.out = nn.Linear(h1, out)\n",
    "\n",
    "        if self.h2 != 0:\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.out = nn.Linear(h2, out)\n",
    "        \n",
    "        #we do it stateless so there is no need for the hidden_state\n",
    "        #self.hidden = None #torch.randn(1, ??, self.hidden) #num_layer, batch, hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _ = x.shape \n",
    "        \n",
    "        x, _ =  self.lstm(x)\n",
    "        x = F.tanh(x[:,-1].view(batch_size, -1))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.h2 != 0:\n",
    "            x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: 2485\n",
      "CNN: 777613\n",
      "GRU: 24519\n",
      "LSTM: 62882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fede/.anaconda/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fede/.anaconda/lib/python3.7/site-packages/torch/serialization.py:453: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "mse_1 = {\n",
    "    \"RNN\": [\"rnn_MSE\"],\n",
    "    \"CNN\": [\"cnn\", \"cnn_stator\"],\n",
    "    \"GRU\": [\"gru\", \"gru_stator\"],\n",
    "    \"LSTM\": [\"lstm\", \"lstm_stator\"]\n",
    "}\n",
    "print_params(mse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## PK Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: 3190\n",
      "CNN: 11840\n",
      "GRU: 7369\n",
      "LSTM: 97249\n"
     ]
    }
   ],
   "source": [
    "custom_loss_1 = {\n",
    "    \"RNN\": [\"rnn_CustomLoss\"],\n",
    "    \"CNN\": [\"cnn_newLoss\"],\n",
    "    \"GRU\": [\"gru_new_loss\"],\n",
    "    \"LSTM\": [\"lstm_new_loss\"]\n",
    "}\n",
    "print_params(custom_loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg_CNN_Net(nn.Module):  # SecondTaskCNN_LSTM, cella 6\n",
    "    def __init__(self, features, seq_len, \n",
    "                 conv1, conv2, kernel1, kernel2,\n",
    "                 h1, h2, out):\n",
    "        super(Reg_CNN_Net, self).__init__()\n",
    "        #self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        #self.conv1 = conv1\n",
    "        self.conv2 = conv2\n",
    "\n",
    "        self.features = features\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.c1 = nn.Conv1d(self.seq_len, conv1, kernel1)\n",
    "        \n",
    "        h0 = outputSize(self.features, kernel1, 1, 0)*conv1\n",
    "        if conv2 != 0:\n",
    "            self.c2 = nn.Conv1d(conv1, conv2, kernel2)\n",
    "        \n",
    "        \n",
    "            h0 = outputSize(outputSize(self.features, kernel1, 1, 0), \n",
    "                            kernel2, 1 ,0)*conv2\n",
    "\n",
    "        self.fc1 = nn.Linear(h0, h1)\n",
    "        \n",
    "\n",
    "        if self.h2 != 0:\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            \n",
    "            self.out = nn.Linear(h2, out)\n",
    "        else:\n",
    "            self.out = nn.Linear(h1, out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _ = x.shape \n",
    "        \n",
    "        x1 = F.relu(self.c1(x))\n",
    "        if self.conv2!=0:\n",
    "            x = F.relu(self.c2(x1))\n",
    "        else:\n",
    "            x = x1\n",
    "        \n",
    "        x = x.view(batch_size,-1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.h2 != 0:\n",
    "            x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net(nn.Module):  # SecondTaskCNN_LSTM, cella 6\n",
    "    def __init__(self, features, hidden, h1, h2, out):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "\n",
    "        self.hidden_size = hidden\n",
    "        self.features = features\n",
    "\n",
    "        self.lstm = nn.LSTM(self.features, self.hidden_size, 1, #feature_size, hidden_size, num_layer\n",
    "                            batch_first = True) \n",
    "        self.fc1 = nn.Linear(self.hidden_size, \n",
    "                                h1)\n",
    "        self.out = nn.Linear(h1, out)\n",
    "\n",
    "        if self.h2 != 0:\n",
    "            self.fc2 = nn.Linear(h1, h2)\n",
    "            self.out = nn.Linear(h2, out)\n",
    "        \n",
    "        #we do it stateless so there is no need for the hidden_state\n",
    "        #self.hidden = None #torch.randn(1, ??, self.hidden) #num_layer, batch, hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _ = x.shape \n",
    "        \n",
    "        x, _ =  self.lstm(x)\n",
    "        x = F.tanh(x[:,-1].view(batch_size, -1))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        if self.h2 != 0:\n",
    "            x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_RNN(torch.nn.Module):  # Rete2, cella 19\n",
    "\n",
    "    def __init__(self, hiddens, recur,\n",
    "                 regressors,\n",
    "                 p_dropout=0.3):\n",
    "        self.recur_input, self.recur_output = recur\n",
    "        # recur_input in [0, len(hiddens))\n",
    "        # recur_output in [-1, len(hiddens)]\n",
    "        assert 0 <= self.recur_input <= len(hiddens)\n",
    "        assert -1 <= self.recur_output < len(hiddens)\n",
    "        assert len(regressors) == 2\n",
    "\n",
    "        super(Custom_RNN, self).__init__()\n",
    "        input_shape = len(FEATURES)  # new task\n",
    "        # input_shape = len(FEATURES) + len(TARGET)  # old task\n",
    "        self.previous_size = hiddens[self.recur_output] \\\n",
    "            if self.recur_output >= 0 else len(TARGET)\n",
    "        \n",
    "        hiddens_in = [input_shape] + hiddens\n",
    "\n",
    "        hiddens_in[self.recur_input] += self.previous_size\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=p_dropout)\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [torch.nn.Linear(in_shape, out_shape) \\\n",
    "             for in_shape, out_shape in zip(hiddens_in, hiddens)])\n",
    "        self.layers_fn = [torch.nn.functional.relu \\\n",
    "                          for _ in self.layers]\n",
    "\n",
    "        regressor_list = []\n",
    "        for features, regressor in zip([1, 3], regressors):\n",
    "            if type(regressor) != list:\n",
    "                regressor = [regressor]\n",
    "            input_shape = [hiddens[-1]] + regressor\n",
    "            y_net = [torch.nn.ReLU() for _ in input_shape + regressor]\n",
    "            y_net[::2] = [torch.nn.Linear(in_shape, out_shape)\n",
    "                          for in_shape, out_shape\n",
    "                          in zip(input_shape, regressor + [features])]\n",
    "            regressor_list.append(torch.nn.Sequential(*y_net))\n",
    "        self.regressors = torch.nn.ModuleList(regressor_list)\n",
    "\n",
    "    def initialize_prev(self, x):\n",
    "        return torch.zeros(len(x), self.previous_size).to(DEVICE)\n",
    "\n",
    "    def forward(self, data, lag=None):\n",
    "        _, l, _ = data.shape\n",
    "        x = data[:, 0, :]  # .reshape(-1, len(FEATURES))\n",
    "        if lag is None:  # first recursion\n",
    "            lag = self.initialize_prev(x)\n",
    "        for i, (layer, fn) in enumerate(zip(self.layers, self.layers_fn)):\n",
    "            if i == self.recur_input:\n",
    "                x = torch.cat([x, lag], axis=1)\n",
    "            x = fn(layer(x))\n",
    "            x = self.dropout(x)\n",
    "            if i == self.recur_output and l > 1:\n",
    "                return self.forward(data[:, 1:, :], x)\n",
    "        x = torch.cat([regressor(x) for regressor in self.regressors],\n",
    "                      axis=1)\n",
    "        if self.recur_output == -1 and l > 1:\n",
    "            return self.forward(data[:, 1:, :], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: 1740\n",
      "CNN: 0.0\n",
      "GRU: 0.0\n",
      "LSTM: 0.0\n"
     ]
    }
   ],
   "source": [
    "mse_2 = {\n",
    "    \"RNN\": [\"rnn2_MSE\"],\n",
    "    \"CNN\": [],\n",
    "    \"GRU\": [],\n",
    "    \"LSTM\": []\n",
    "}\n",
    "print_params(mse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PK Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN: 1740\n",
      "CNN: 0.0\n",
      "GRU: 0.0\n",
      "LSTM: 0.0\n"
     ]
    }
   ],
   "source": [
    "custom_loss_2 = {\n",
    "    \"RNN\": [\"rnn2_Custom\"],\n",
    "    \"CNN\": [],\n",
    "    \"GRU\": [],\n",
    "    \"LSTM\": []\n",
    "}\n",
    "print_params(custom_loss_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Untitled1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
